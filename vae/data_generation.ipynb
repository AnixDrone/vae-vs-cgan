{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tignjatov/anaconda3/envs/ml/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS_IMG = 1\n",
    "IMG_SIZE = 32\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_data_path = '../fake_mnist_datasets/cvae/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 latent_dim,\n",
    "                 num_classes,\n",
    "                 image_size,\n",
    "                 device,\n",
    "                 hidden_dims = None,\n",
    "                 features_d = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_channel = in_channels\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.embed_class = nn.Embedding(num_classes, self.image_size * self.image_size)\n",
    "        #self.embed_data = nn.Conv2d(in_channels, in_channels,1)\n",
    "        \n",
    "        \n",
    "            \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels+1, features_d, 4, 2, 1), # 64 -> 32\n",
    "            nn.InstanceNorm2d(features_d, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._encoder_block(features_d, features_d * 2, 4, 2, 1),# 32 -> 16\n",
    "            self._encoder_block(features_d * 2, features_d * 4, 4, 2, 1),# 16 - > 8\n",
    "            self._encoder_block(features_d * 4, features_d * 8, 4, 2, 1), # 8 -> 4\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(features_d * 8 * 2 * 2, latent_dim)\n",
    "        self.fc_var = nn.Linear(features_d * 8 * 2 * 2, latent_dim)\n",
    "        \n",
    "        \n",
    "        self.decoder_input = nn.Linear(latent_dim + 1, features_d * 8 * 2 * 2)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            self._decoder_bloc(features_d * 8, features_d * 4, 4, 2, 1),# 32 -> 16\n",
    "            self._decoder_bloc(features_d * 4, features_d * 2, 4, 2, 1),# 16 - > 8\n",
    "            self._decoder_bloc(features_d * 2, features_d, 4, 2, 1), # 8 -> 4\n",
    "            nn.ConvTranspose2d(features_d, in_channels, 4, 2, 1), # 64 -> 32\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels, in_channels, 1), # 32 -> 64\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def _encoder_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "    \n",
    "    def _decoder_bloc(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def encode(self,input):\n",
    "        result = self.encoder(input)\n",
    "        #print(result.shape)\n",
    "        result = result.flatten(1)\n",
    "        \n",
    "        if torch.isnan(result).any().item():\n",
    "            print(result)\n",
    "        \n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "        \n",
    "        return mu, log_var\n",
    "    \n",
    "    def decode(self,z):\n",
    "        result  = self.decoder_input(z)\n",
    "        #print(result.shape)\n",
    "        result  = result.view(-1,512,2,2)\n",
    "        #print(result.shape)\n",
    "        result = self.decoder(result)\n",
    "        return result\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "        \n",
    "    \n",
    "    def forward(self,input,label):\n",
    "        embedded_class = self.embed_class(label)\n",
    "        embedded_class = embedded_class.view(label.size(0),1,self.image_size,self.image_size)\n",
    "        #embedded_input = self.embed_data(input) \n",
    "        \n",
    "        x = torch.cat([input,embedded_class],dim=1)\n",
    "        \n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        lab = label.unsqueeze(1)\n",
    "        \n",
    "        z = torch.cat([z,lab],dim=1)\n",
    "        \n",
    "        return self.decode(z), mu, log_var\n",
    "    \n",
    "    def sample(self, n,label):\n",
    "        label = label.unsqueeze(1)\n",
    "        z = torch.randn(n, self.latent_dim).to(device)\n",
    "        z = torch.cat([z,label],dim=1)\n",
    "        #z = z.to(self.device)\n",
    "        \n",
    "        return self.decode(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConditionalVAE(in_channels=CHANNELS_IMG,latent_dim=2,num_classes=NUM_CLASSES,image_size=IMG_SIZE,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models_save/40_epochs_cvae_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionalVAE(\n",
       "  (embed_class): Embedding(10, 1024)\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(2, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  (fc_var): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  (decoder_input): Linear(in_features=3, out_features=2048, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(10)):\n",
    "    if not os.path.exists(os.path.join(cvae_data_path, str(i))):\n",
    "        os.mkdir(os.path.join(cvae_data_path, str(i)))\n",
    "        \n",
    "    #img_noise = torch.randn(100, Z_DIM, 1, 1).to(device)\n",
    "    label = torch.tensor([i]*100).to(device)\n",
    "    gen_img = model.sample(100,label)\n",
    "    \n",
    "    [torchvision.utils.save_image(gen_img[j], os.path.join(cvae_data_path, str(i),f'gen_img_{j}.png'), normalize=True) for j in range(100)]\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e52387336156db6d881f08b29d2996eae333b560bff7fc97cb6248ede68f9d47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
